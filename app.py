from fastapi import FastAPI, Header, HTTPException, UploadFile, File, HTTPException
from pydantic import BaseModel
import mlflow
from mlflow.exceptions import MlflowException, RestException
import mlflow.sklearn
import numpy as np
from contextlib import asynccontextmanager
from mlflow.tracking import MlflowClient
import threading
import time
import random


is_reloading = False
model_lock = threading.Lock()
model = None
loaded_version = None  # ì„œë²„ê°€ í˜„ì¬ ë“¤ê³  ìˆëŠ” ëª¨ë¸ ë²„ì „ (ìƒíƒœ) -> ì™œ ì´ ìœ„ì¹˜ì¸ì§€ : ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ” ë™ì•ˆ ê³„ì† ìœ ì§€ë¼ì•¼ í•˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸
MODEL_URI = "models:/IrisClassifier@production"
mlflow.set_tracking_uri("http://localhost:5000")

class DummyYoloModel:
    def __init__(self):
        # YOLO ê°€ì¤‘ì¹˜ ë¡œë”© í‰ë‚´
        print("ğŸ§  Initializing Dummy YOLO model...")
        time.sleep(5)
        print("ğŸ§  Dummy YOLO model ready")

    def predict(self, image_bytes: bytes):
        # YOLO ì¶”ë¡  í‰ë‚´
        time.sleep(0.3)

        # ê²°ê³¼ í‰ë‚´
        return [
            {
                "class": "object",
                "confidence": round(random.uniform(0.5, 0.9), 2),
                "bbox": [100,120,300,350] # x1,y1,x2,y2
            }
        ]


def load_model():
    global model, loaded_version, is_reloading
    is_reloading = True
    try:
        with model_lock:
            try:
                # 1) ë¨¼ì € ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ alias ë²„ì „ í™•ì¸
                mv = MlflowClient().get_model_version_by_alias("IrisClassifier", "production")

                # 2) ì„±ê³µí–ˆì„ ë•Œë§Œ ëª¨ë¸ ì¤€ë¹„
                time.sleep(3)
                model = DummyYoloModel()
                loaded_version = mv.version
                print(f"ğŸ§· Loaded model: Dummy YOLO (alias=production, version={mv.version})")

            except RestException as e:
                # Registered Modelì´ ì—†ê±°ë‚˜ aliasê°€ ì—†ëŠ” ê²½ìš° ì—¬ê¸°ë¡œ ì˜´
                model = None
                loaded_version = "none"
                print(f"ğŸŸ  Model not ready (registry): {e}")

    finally:
        is_reloading = False



@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸ”µ Loading model...")
    load_model()
    if model is None:
        print("ğŸŸ¡ Model NOT loaded (ready=false)")
    else:
        print("ğŸŸ¢ Model loaded (ready=true)")
    yield
    print("ğŸ”´ App shutdown")


app = FastAPI(lifespan=lifespan)


### ê¸°ì¡´ ìŠ¤í…ì—ì„œ ì‚¬ìš©í•˜ë˜ ì‹¤ìˆ˜ ì˜ˆì¸¡ ë¶€ë¶„ ###
# class PredictRequest(BaseModel):
#     x1: float
#     x2: float
#     x3: float
#     x4: float


@app.get("/ping")
def ping():
    return {"status": "ok", "model_version" : loaded_version}

ADMIN_TOKEN = "dev-only-token"

@app.get("/live")
def live():
    return {"status": "alive"}


@app.get("/ready")
def ready():
    if model is None:
        return {"status": False, "reason": "model_not_loaded"}
    return {"status": True, "model_version": loaded_version}


@app.post("/admin/reload")
def admin_reload(x_admin_token: str | None = Header(default=None)):
    if x_admin_token != ADMIN_TOKEN:
        raise HTTPException(status_code = 401, detail = "Unauthorized")

    load_model()
    return {
        "status" : "reloaded",
        "model_version" : loaded_version
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    if is_reloading:
        raise  HTTPException(status_code=503, detail="Model is reloading. Try again.")
    # íŒŒì¼ì„ ì½ì–´ì„œ ë°”ì´íŠ¸ë¡œ ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ëŠ” ì¶”ë¡  ì§„í–‰ ì•ˆí•¨)
    image_bytes = await file.read()

    with model_lock:
        detections = model.predict(image_bytes)

    # ì§€ê¸ˆ ë‹¨ê³„ì—ì„œëŠ” "í˜•íƒœë§Œ" YOLO ìŠ¤íƒ€ì¼ë¡œ ë°˜í™˜
    return {
        "filename": file.filename,
        "content_type": file.content_type,
        "bytes": len(image_bytes),
        "model_version": loaded_version,
        "detections": detections
    }


